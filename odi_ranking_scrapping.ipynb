{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISB Assignment\n",
    "### - By Pranshu Bansal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrap the table data from the Beautiful Soup object and build and save the data into a CSV file\n",
    "def scrap_data(table, category, day_path, date):\n",
    "    \n",
    "    # Creating lists to store the entries of the table\n",
    "    position = []\n",
    "    names = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    best_ratings = []\n",
    "    dates = []\n",
    "\n",
    "    # First row of the table is in banner form on the website, so it has to be treated seperately\n",
    "    pos = 1\n",
    "    # Defining the table row that has the entry of the first player\n",
    "    first_row = table.find('tr', class_ = \"rankings-block__banner\")     \n",
    "    \n",
    "    # Finding and storing all the table datas from the first row that has the information about the player (like name, team, rating, best_rating)\n",
    "    name = first_row.find('td', class_ = \"rankings-block__top-player-container\").text.strip()\n",
    "    team = first_row.find_all('td')[2].text.strip()\n",
    "    rating = first_row.find('td', class_ = \"u-text-left\").text.strip()\n",
    "    best_rating = first_row.find('td', class_ = \"u-text-right u-hide-phablet u-overflow-hidden\").text.strip()\n",
    "\n",
    "    # Adding the first player's information in the lists\n",
    "    position.append(pos)\n",
    "    names.append(name)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "    best_ratings.append(best_rating)\n",
    "    dates.append(date)\n",
    "    \n",
    "    # Finding and defining all the rows that has information about all the rest 99 players\n",
    "    rows = table.find_all('tr', class_ = \"table-body\")\n",
    "\n",
    "    # Iterating over all the player rows\n",
    "    for row in rows:\n",
    "        # Finding and storing all the table datas from each row that has the information about the player (like name, team, rating, best_rating)\n",
    "        pos += 1\n",
    "        name = row.find('td', class_ = \"table-body__cell rankings-table__name name\").text.strip()\n",
    "        team = row.find('td', class_ = \"table-body__cell nationality-logo rankings-table__team\").text.strip()\n",
    "        rating = row.find('td', class_ = \"table-body__cell rating\").text.strip()\n",
    "        best_rating = row.find('td', class_ = \"table-body__cell u-text-right u-hide-phablet\").text.strip()\n",
    "\n",
    "        # Adding each player's information in the lists\n",
    "        position.append(pos)\n",
    "        names.append(name)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "        best_ratings.append(best_rating)\n",
    "        dates.append(date)\n",
    "\n",
    "    # Creating a dictionary that has the key as the name of the column and value as the players' information lists\n",
    "    dict = {\n",
    "        'POS' : position,\n",
    "        'NAME' : names,\n",
    "        'TEAM' : teams,\n",
    "        'RATING' : ratings,\n",
    "        'CAREER BEST RATING' : best_ratings,\n",
    "        'DATE' : dates\n",
    "    }\n",
    "    \n",
    "    # Converting the dictionary into pandas DataFrame that can be stored in a csv file\n",
    "    df = pd.DataFrame(dict)\n",
    "    \n",
    "    # Storing the DataFrame into csv file. Defining the path where the csv file would be stored with the name as category of the player (batting, bowling, all_rounder)\n",
    "    df.to_csv(day_path + '/' + category + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate over all the days, making folder structure, making requests from the particular website to find and build ranking table\n",
    "def build_data():\n",
    "    \n",
    "    # Defining the start and end dates which signifies the timeline through which we need to iterate over\n",
    "    start_date = datetime.date(1971, 2, 1)     # Filter in the website starts from February 1, 1971, so making the start_date as such\n",
    "    end_date = datetime.date(2022, 1, 4)\n",
    "\n",
    "    # update is the stride we need to take while iterating. Here, update is set as 1 day which means iteration would occur day by day\n",
    "    update = datetime.timedelta(days = 1)\n",
    "    \n",
    "    # Defining mon dictionary that has key as month number and value as the month name. Used for naming the folder with the month name\n",
    "    mon = {1:'January', 2:'February', 3:'March', 4:'April', 5:'May', 6:'June', 7:'July', 8:'August', 9:'September', 10:'October', 11:'November', 12:'December'}\n",
    "\n",
    "    # Making a folder named 'ODI Data'\n",
    "#     os.mkdir('ODI Data')\n",
    "    \n",
    "    # Defining the base urls for the batting, bowling and all-rounder websites. It could be joined with any date to get the player rankings of that particular date\n",
    "    base_url_bat = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting?at='\n",
    "    base_url_ball = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling?at='\n",
    "    base_url_all = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/all-rounder?at='\n",
    "\n",
    "    # Making a previous date that could be used as pointer to make new folders\n",
    "    prev_date = start_date - update\n",
    "    \n",
    "    # Making a bool check that is used to make the first year's folder (1971)\n",
    "    first_year = True\n",
    "    \n",
    "    # Iterate over each date from start_date to end_date\n",
    "    while start_date <= end_date:\n",
    "        \n",
    "        # To make the year folder whenever the start_date year is not equal to prev_date year (it means the year has changed from the prev) or if it is the first year\n",
    "        if first_year==True or start_date.year != prev_date.year: \n",
    "            year_path = os.path.join('ODI Data/', str(start_date.year))\n",
    "            os.mkdir(year_path)\n",
    "            first_year = False\n",
    "        \n",
    "        # To make the month folder inside the year folder when the start_date month is not equal to prev_date month. Month name would be the name of the folder\n",
    "        if start_date.month != prev_date.month:\n",
    "            mon_path = os.path.join(year_path, str(start_date.month) + '. ' + str(mon[start_date.month]))\n",
    "            os.mkdir(mon_path)\n",
    "            \n",
    "        # To change the name of the day if it is single digit in string format(Eg: 5 is converted into 05)\n",
    "        if start_date.day < 10:\n",
    "            d = '0'+ str(start_date.day)\n",
    "        else:\n",
    "            d = str(start_date.day)\n",
    "        \n",
    "        # To change the name of the month if it is single digit in string format(Eg: 4 is converted into 04)\n",
    "        if start_date.month < 10:\n",
    "            m = '0' + str(start_date.month)\n",
    "        else:\n",
    "            m = str(start_date.month)\n",
    "        \n",
    "        # To make the day folder inside the month folder with the name in format - 'DDMMYYY'\n",
    "        day_path = os.path.join(mon_path, d + m + str(start_date.year))\n",
    "        os.mkdir(day_path)\n",
    "        \n",
    "        # _____________________BATTING_______________________________\n",
    "        \n",
    "        # Make the complete URL with the base URL and combining it with the date\n",
    "        url_bat = base_url_bat + str(start_date)\n",
    "        \n",
    "        # Make a GET Request to the URL \n",
    "        bat = requests.get(url_bat)\n",
    "        \n",
    "        # Creating the Beautiful Soup Object\n",
    "        soup = BeautifulSoup(bat.text)\n",
    "        \n",
    "        # Finding the ranking table from the soup object\n",
    "        bat_table = soup.find('table', class_ = 'table rankings-table')\n",
    "        \n",
    "        # Calling the scrap_data function. Giving it the batting table, category as batting, path where batting.csv would be stored and current date as parameters.\n",
    "        scrap_data(bat_table, \"batting\", day_path, str(start_date))\n",
    "        \n",
    "        \n",
    "        # _____________________BALLING_________________________________\n",
    "        \n",
    "        # Make the complete URL with the base URL and combining it with the date\n",
    "        url_ball = base_url_ball + str(start_date)\n",
    "        \n",
    "        # Make a GET Request to the URL\n",
    "        ball = requests.get(url_ball)\n",
    "        \n",
    "        # Creating the Beautiful Soup Object\n",
    "        soup = BeautifulSoup(ball.text)\n",
    "        \n",
    "        # Finding the ranking table from the soup object\n",
    "        ball_table = soup.find('table', class_ = 'table rankings-table')\n",
    "        \n",
    "        # Calling the scrap_data function. Giving it the balling table, category as balling, path where balling.csv would be stored and current date as parameters.\n",
    "        scrap_data(ball_table, \"balling\", day_path, str(start_date))\n",
    "        \n",
    "        \n",
    "        # ____________________ALL ROUNDER_________________________________\n",
    "        \n",
    "        # Make the complete URL with the base URL and combining it with the date\n",
    "        url_all = base_url_all + str(start_date)\n",
    "        \n",
    "        # Make a GET Request to the URL\n",
    "        all_r = requests.get(url_all)\n",
    "        \n",
    "        # Creating the Beautiful Soup Object\n",
    "        soup = BeautifulSoup(all_r.text)\n",
    "        \n",
    "        # Finding the ranking table from the soup object\n",
    "        all_table = soup.find('table', class_ = 'table rankings-table')\n",
    "        \n",
    "        # Calling the scrap_data function. Giving it the all_rounder table, category as all_rounder, path where all_rounder.csv would be stored and current date as parameters.\n",
    "        scrap_data(all_table, \"all_rounder\", day_path, str(start_date))\n",
    "\n",
    "        \n",
    "        # Updating previous date as start date (current date)\n",
    "        prev_date = start_date\n",
    "        # Incrementing start_date by a day\n",
    "        start_date += update\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the build_data function\n",
    "build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
